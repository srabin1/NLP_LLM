{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d707dbc3",
   "metadata": {},
   "source": [
    "# XLNET"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1599ea2f",
   "metadata": {},
   "source": [
    "## unlike BERT that uses different mask for some words, XLNET provide all words in advance and different permutation for them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "75ae110f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting clean-text\n",
      "  Downloading clean_text-0.6.0-py3-none-any.whl (11 kB)\n",
      "Collecting emoji<2.0.0,>=1.0.0\n",
      "  Downloading emoji-1.7.0.tar.gz (175 kB)\n",
      "Collecting ftfy<7.0,>=6.0\n",
      "  Downloading ftfy-6.2.3-py3-none-any.whl (43 kB)\n",
      "Collecting wcwidth<0.3.0,>=0.2.12\n",
      "  Downloading wcwidth-0.2.13-py2.py3-none-any.whl (34 kB)\n",
      "Building wheels for collected packages: emoji\n",
      "  Building wheel for emoji (setup.py): started\n",
      "  Building wheel for emoji (setup.py): finished with status 'done'\n",
      "  Created wheel for emoji: filename=emoji-1.7.0-py3-none-any.whl size=171046 sha256=740521489f9c9a9b5d1a8c126e23e25384963573c308fd8c7906b56345761806\n",
      "  Stored in directory: c:\\users\\sanaz\\appdata\\local\\pip\\cache\\wheels\\fa\\7a\\e9\\22dd0515e1bad255e51663ee513a2fa839c95934c5fc301090\n",
      "Successfully built emoji\n",
      "Installing collected packages: wcwidth, ftfy, emoji, clean-text\n",
      "  Attempting uninstall: wcwidth\n",
      "    Found existing installation: wcwidth 0.2.5\n",
      "    Uninstalling wcwidth-0.2.5:\n",
      "      Successfully uninstalled wcwidth-0.2.5\n",
      "Successfully installed clean-text-0.6.0 emoji-1.7.0 ftfy-6.2.3 wcwidth-0.2.13\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install clean-text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f02665d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting datasets\n",
      "  Downloading datasets-2.20.0-py3-none-any.whl (547 kB)\n",
      "Collecting fsspec[http]<=2024.5.0,>=2023.1.0\n",
      "  Downloading fsspec-2024.5.0-py3-none-any.whl (316 kB)\n",
      "Collecting pyarrow>=15.0.0\n",
      "  Downloading pyarrow-17.0.0-cp39-cp39-win_amd64.whl (25.1 MB)\n",
      "Requirement already satisfied: pandas in c:\\users\\sanaz\\anaconda3\\lib\\site-packages (from datasets) (1.4.2)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\sanaz\\anaconda3\\lib\\site-packages (from datasets) (1.26.4)\n",
      "Collecting tqdm>=4.66.3\n",
      "  Downloading tqdm-4.66.5-py3-none-any.whl (78 kB)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\sanaz\\anaconda3\\lib\\site-packages (from datasets) (6.0)\n",
      "Requirement already satisfied: aiohttp in c:\\users\\sanaz\\anaconda3\\lib\\site-packages (from datasets) (3.10.2)\n",
      "Collecting pyarrow-hotfix\n",
      "  Downloading pyarrow_hotfix-0.6-py3-none-any.whl (7.9 kB)\n",
      "Collecting multiprocess\n",
      "  Downloading multiprocess-0.70.16-py39-none-any.whl (133 kB)\n",
      "Requirement already satisfied: filelock in c:\\users\\sanaz\\anaconda3\\lib\\site-packages (from datasets) (3.6.0)\n",
      "Requirement already satisfied: packaging in c:\\users\\sanaz\\anaconda3\\lib\\site-packages (from datasets) (24.1)\n",
      "Requirement already satisfied: huggingface-hub>=0.21.2 in c:\\users\\sanaz\\anaconda3\\lib\\site-packages (from datasets) (0.24.5)\n",
      "Collecting xxhash\n",
      "  Downloading xxhash-3.4.1-cp39-cp39-win_amd64.whl (29 kB)\n",
      "Collecting requests>=2.32.2\n",
      "  Downloading requests-2.32.3-py3-none-any.whl (64 kB)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in c:\\users\\sanaz\\anaconda3\\lib\\site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\sanaz\\anaconda3\\lib\\site-packages (from aiohttp->datasets) (5.1.0)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in c:\\users\\sanaz\\anaconda3\\lib\\site-packages (from aiohttp->datasets) (2.3.5)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\sanaz\\anaconda3\\lib\\site-packages (from aiohttp->datasets) (1.2.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\users\\sanaz\\anaconda3\\lib\\site-packages (from aiohttp->datasets) (1.6.3)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in c:\\users\\sanaz\\anaconda3\\lib\\site-packages (from aiohttp->datasets) (4.0.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\sanaz\\anaconda3\\lib\\site-packages (from aiohttp->datasets) (21.4.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\sanaz\\anaconda3\\lib\\site-packages (from aiohttp->datasets) (1.2.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.5 in c:\\users\\sanaz\\anaconda3\\lib\\site-packages (from async-timeout<5.0,>=4.0->aiohttp->datasets) (4.12.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\sanaz\\anaconda3\\lib\\site-packages (from requests>=2.32.2->datasets) (2021.10.8)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\sanaz\\anaconda3\\lib\\site-packages (from requests>=2.32.2->datasets) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\sanaz\\anaconda3\\lib\\site-packages (from requests>=2.32.2->datasets) (3.3)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\sanaz\\anaconda3\\lib\\site-packages (from requests>=2.32.2->datasets) (1.26.9)\n",
      "Requirement already satisfied: colorama in c:\\users\\sanaz\\anaconda3\\lib\\site-packages (from tqdm>=4.66.3->datasets) (0.4.4)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\sanaz\\anaconda3\\lib\\site-packages (from pandas->datasets) (2021.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in c:\\users\\sanaz\\anaconda3\\lib\\site-packages (from pandas->datasets) (2.8.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\sanaz\\anaconda3\\lib\\site-packages (from python-dateutil>=2.8.1->pandas->datasets) (1.16.0)\n",
      "Installing collected packages: tqdm, requests, fsspec, xxhash, pyarrow-hotfix, pyarrow, multiprocess, datasets\n",
      "  Attempting uninstall: tqdm\n",
      "    Found existing installation: tqdm 4.64.0\n",
      "    Uninstalling tqdm-4.64.0:\n",
      "      Successfully uninstalled tqdm-4.64.0\n",
      "  Attempting uninstall: requests\n",
      "    Found existing installation: requests 2.27.1\n",
      "    Uninstalling requests-2.27.1:\n",
      "      Successfully uninstalled requests-2.27.1\n",
      "  Attempting uninstall: fsspec\n",
      "    Found existing installation: fsspec 2024.6.1\n",
      "    Uninstalling fsspec-2024.6.1:\n",
      "      Successfully uninstalled fsspec-2024.6.1\n",
      "Successfully installed datasets-2.20.0 fsspec-2024.5.0 multiprocess-0.70.16 pyarrow-17.0.0 pyarrow-hotfix-0.6 requests-2.32.3 tqdm-4.66.5 xxhash-3.4.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "conda-repo-cli 1.0.4 requires pathlib, which is not installed.\n"
     ]
    }
   ],
   "source": [
    "pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "18c77baa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting evaluate\n",
      "  Downloading evaluate-0.4.2-py3-none-any.whl (84 kB)\n",
      "Requirement already satisfied: pandas in c:\\users\\sanaz\\anaconda3\\lib\\site-packages (from evaluate) (1.4.2)\n",
      "Requirement already satisfied: xxhash in c:\\users\\sanaz\\anaconda3\\lib\\site-packages (from evaluate) (3.4.1)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\sanaz\\anaconda3\\lib\\site-packages (from evaluate) (1.26.4)\n",
      "Requirement already satisfied: datasets>=2.0.0 in c:\\users\\sanaz\\anaconda3\\lib\\site-packages (from evaluate) (2.20.0)\n",
      "Requirement already satisfied: huggingface-hub>=0.7.0 in c:\\users\\sanaz\\anaconda3\\lib\\site-packages (from evaluate) (0.24.5)\n",
      "Requirement already satisfied: packaging in c:\\users\\sanaz\\anaconda3\\lib\\site-packages (from evaluate) (24.1)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in c:\\users\\sanaz\\anaconda3\\lib\\site-packages (from evaluate) (4.66.5)\n",
      "Requirement already satisfied: multiprocess in c:\\users\\sanaz\\anaconda3\\lib\\site-packages (from evaluate) (0.70.16)\n",
      "Requirement already satisfied: fsspec[http]>=2021.05.0 in c:\\users\\sanaz\\anaconda3\\lib\\site-packages (from evaluate) (2024.5.0)\n",
      "Requirement already satisfied: requests>=2.19.0 in c:\\users\\sanaz\\anaconda3\\lib\\site-packages (from evaluate) (2.32.3)\n",
      "Requirement already satisfied: dill in c:\\users\\sanaz\\anaconda3\\lib\\site-packages (from evaluate) (0.3.8)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\sanaz\\anaconda3\\lib\\site-packages (from datasets>=2.0.0->evaluate) (6.0)\n",
      "Requirement already satisfied: aiohttp in c:\\users\\sanaz\\anaconda3\\lib\\site-packages (from datasets>=2.0.0->evaluate) (3.10.2)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in c:\\users\\sanaz\\anaconda3\\lib\\site-packages (from datasets>=2.0.0->evaluate) (17.0.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\sanaz\\anaconda3\\lib\\site-packages (from datasets>=2.0.0->evaluate) (3.6.0)\n",
      "Requirement already satisfied: pyarrow-hotfix in c:\\users\\sanaz\\anaconda3\\lib\\site-packages (from datasets>=2.0.0->evaluate) (0.6)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\sanaz\\anaconda3\\lib\\site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\sanaz\\anaconda3\\lib\\site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.2.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\sanaz\\anaconda3\\lib\\site-packages (from aiohttp->datasets>=2.0.0->evaluate) (21.4.0)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in c:\\users\\sanaz\\anaconda3\\lib\\site-packages (from aiohttp->datasets>=2.0.0->evaluate) (4.0.1)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in c:\\users\\sanaz\\anaconda3\\lib\\site-packages (from aiohttp->datasets>=2.0.0->evaluate) (2.3.5)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\sanaz\\anaconda3\\lib\\site-packages (from aiohttp->datasets>=2.0.0->evaluate) (5.1.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\users\\sanaz\\anaconda3\\lib\\site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.6.3)\n",
      "Requirement already satisfied: typing-extensions>=3.6.5 in c:\\users\\sanaz\\anaconda3\\lib\\site-packages (from async-timeout<5.0,>=4.0->aiohttp->datasets>=2.0.0->evaluate) (4.12.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\sanaz\\anaconda3\\lib\\site-packages (from requests>=2.19.0->evaluate) (2021.10.8)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\sanaz\\anaconda3\\lib\\site-packages (from requests>=2.19.0->evaluate) (3.3)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\sanaz\\anaconda3\\lib\\site-packages (from requests>=2.19.0->evaluate) (1.26.9)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\sanaz\\anaconda3\\lib\\site-packages (from requests>=2.19.0->evaluate) (2.0.4)\n",
      "Requirement already satisfied: colorama in c:\\users\\sanaz\\anaconda3\\lib\\site-packages (from tqdm>=4.62.1->evaluate) (0.4.4)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\sanaz\\anaconda3\\lib\\site-packages (from pandas->evaluate) (2021.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in c:\\users\\sanaz\\anaconda3\\lib\\site-packages (from pandas->evaluate) (2.8.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\sanaz\\anaconda3\\lib\\site-packages (from python-dateutil>=2.8.1->pandas->evaluate) (1.16.0)\n",
      "Installing collected packages: evaluate\n",
      "Successfully installed evaluate-0.4.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a1ad183a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sanaz\\anaconda3\\lib\\site-packages\\scipy\\__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.26.4\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\sanaz\\anaconda3\\lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from cleantext import clean\n",
    "import re\n",
    "from transformers import XLNetTokenizer, XLNetForSequenceClassification, TrainingArguments, Trainer, pipeline\n",
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import datasets \n",
    "import evaluate\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b57d7786",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train = pd.read_csv('emotion-labels-train.csv')\n",
    "data_test = pd.read_csv('emotion-labels-test.csv')\n",
    "data_val = pd.read_csv('emotion-labels-val.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f642d8b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Just got back from seeing @GaryDelaney in Burs...</td>\n",
       "      <td>joy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Oh dear an evening of absolute hilarity I don'...</td>\n",
       "      <td>joy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Been waiting all week for this game ❤️❤️❤️ #ch...</td>\n",
       "      <td>joy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>@gardiner_love : Thank you so much, Gloria! Yo...</td>\n",
       "      <td>joy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I feel so blessed to work with the family that...</td>\n",
       "      <td>joy</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text label\n",
       "0  Just got back from seeing @GaryDelaney in Burs...   joy\n",
       "1  Oh dear an evening of absolute hilarity I don'...   joy\n",
       "2  Been waiting all week for this game ❤️❤️❤️ #ch...   joy\n",
       "3  @gardiner_love : Thank you so much, Gloria! Yo...   joy\n",
       "4  I feel so blessed to work with the family that...   joy"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "596e8f6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.concat([data_train, data_test, data_val], ignore_index = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a4e7fe29",
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove emojis\n",
    "data['text_clean'] = data['text'].apply(lambda x: clean(x, no_emoji=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "79dc1ba2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove punctuation\n",
    "data['text_clean'] = data['text_clean'].apply(lambda x: re.sub('@[^\\s]+', '', x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e350b712",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>text_clean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Just got back from seeing @GaryDelaney in Burs...</td>\n",
       "      <td>joy</td>\n",
       "      <td>just got back from seeing  in burslem. amazing...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Oh dear an evening of absolute hilarity I don'...</td>\n",
       "      <td>joy</td>\n",
       "      <td>oh dear an evening of absolute hilarity i don'...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Been waiting all week for this game ❤️❤️❤️ #ch...</td>\n",
       "      <td>joy</td>\n",
       "      <td>been waiting all week for this game #cheer #fr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>@gardiner_love : Thank you so much, Gloria! Yo...</td>\n",
       "      <td>joy</td>\n",
       "      <td>: thank you so much, gloria! you're so sweet,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I feel so blessed to work with the family that...</td>\n",
       "      <td>joy</td>\n",
       "      <td>i feel so blessed to work with the family that...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Today I reached 1000 subscribers on YT!! , #go...</td>\n",
       "      <td>joy</td>\n",
       "      <td>today i reached 1000 subscribers on yt!! , #go...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>@Singaholic121 Good morning, love! Happy first...</td>\n",
       "      <td>joy</td>\n",
       "      <td>good morning, love! happy first day of fall. ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>#BridgetJonesBaby is the best thing I've seen ...</td>\n",
       "      <td>joy</td>\n",
       "      <td>#bridgetjonesbaby is the best thing i've seen ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Just got back from seeing @GaryDelaney in Burs...</td>\n",
       "      <td>joy</td>\n",
       "      <td>just got back from seeing  in burslem. amazing...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>@IndyMN I thought the holidays could not get a...</td>\n",
       "      <td>joy</td>\n",
       "      <td>i thought the holidays could not get any more...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>I'm just still . So happy .\\nA blast</td>\n",
       "      <td>joy</td>\n",
       "      <td>i'm just still . so happy .\\na blast</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>It's meant to be!! #happy #happy</td>\n",
       "      <td>joy</td>\n",
       "      <td>it's meant to be!! #happy #happy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>💥⚖️Yeah‼️ PAUL‼️⚖️💥  #glorious #BB18</td>\n",
       "      <td>joy</td>\n",
       "      <td>yeah!! paul!! #glorious #bb18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>My morning started off amazing!! Hopefully the...</td>\n",
       "      <td>joy</td>\n",
       "      <td>my morning started off amazing!! hopefully the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>😱 @cailamarsai you've had me 😂 😂 the whole tim...</td>\n",
       "      <td>joy</td>\n",
       "      <td>you've had me the whole time watching  after ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>@iamTinaDatta love you so much #smile 😊😊</td>\n",
       "      <td>joy</td>\n",
       "      <td>love you so much #smile</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>@WyoWiseGuy @LivingVertical however, REI did o...</td>\n",
       "      <td>joy</td>\n",
       "      <td>however, rei did offer me the job today as w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>2 days until #GoPackGo and 23 days until #GoGi...</td>\n",
       "      <td>joy</td>\n",
       "      <td>2 days until #gopackgo and 23 days until #gogi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>@TheMandyMoore You are beyond wonderful.  Your...</td>\n",
       "      <td>joy</td>\n",
       "      <td>you are beyond wonderful. your singing prowes...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>@luckiiCHARM_ Luckii, I'm changing in so many ...</td>\n",
       "      <td>joy</td>\n",
       "      <td>luckii, i'm changing in so many ways bc of hi...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 text label  \\\n",
       "0   Just got back from seeing @GaryDelaney in Burs...   joy   \n",
       "1   Oh dear an evening of absolute hilarity I don'...   joy   \n",
       "2   Been waiting all week for this game ❤️❤️❤️ #ch...   joy   \n",
       "3   @gardiner_love : Thank you so much, Gloria! Yo...   joy   \n",
       "4   I feel so blessed to work with the family that...   joy   \n",
       "5   Today I reached 1000 subscribers on YT!! , #go...   joy   \n",
       "6   @Singaholic121 Good morning, love! Happy first...   joy   \n",
       "7   #BridgetJonesBaby is the best thing I've seen ...   joy   \n",
       "8   Just got back from seeing @GaryDelaney in Burs...   joy   \n",
       "9   @IndyMN I thought the holidays could not get a...   joy   \n",
       "10               I'm just still . So happy .\\nA blast   joy   \n",
       "11                   It's meant to be!! #happy #happy   joy   \n",
       "12               💥⚖️Yeah‼️ PAUL‼️⚖️💥  #glorious #BB18   joy   \n",
       "13  My morning started off amazing!! Hopefully the...   joy   \n",
       "14  😱 @cailamarsai you've had me 😂 😂 the whole tim...   joy   \n",
       "15           @iamTinaDatta love you so much #smile 😊😊   joy   \n",
       "16  @WyoWiseGuy @LivingVertical however, REI did o...   joy   \n",
       "17  2 days until #GoPackGo and 23 days until #GoGi...   joy   \n",
       "18  @TheMandyMoore You are beyond wonderful.  Your...   joy   \n",
       "19  @luckiiCHARM_ Luckii, I'm changing in so many ...   joy   \n",
       "\n",
       "                                           text_clean  \n",
       "0   just got back from seeing  in burslem. amazing...  \n",
       "1   oh dear an evening of absolute hilarity i don'...  \n",
       "2   been waiting all week for this game #cheer #fr...  \n",
       "3    : thank you so much, gloria! you're so sweet,...  \n",
       "4   i feel so blessed to work with the family that...  \n",
       "5   today i reached 1000 subscribers on yt!! , #go...  \n",
       "6    good morning, love! happy first day of fall. ...  \n",
       "7   #bridgetjonesbaby is the best thing i've seen ...  \n",
       "8   just got back from seeing  in burslem. amazing...  \n",
       "9    i thought the holidays could not get any more...  \n",
       "10               i'm just still . so happy .\\na blast  \n",
       "11                   it's meant to be!! #happy #happy  \n",
       "12                      yeah!! paul!! #glorious #bb18  \n",
       "13  my morning started off amazing!! hopefully the...  \n",
       "14   you've had me the whole time watching  after ...  \n",
       "15                            love you so much #smile  \n",
       "16    however, rei did offer me the job today as w...  \n",
       "17  2 days until #gopackgo and 23 days until #gogi...  \n",
       "18   you are beyond wonderful. your singing prowes...  \n",
       "19   luckii, i'm changing in so many ways bc of hi...  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0857517a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "data['label'].value_counts().plot(kind=\"bar\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8df73d4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "g = data.groupby('label')\n",
    "data = pd.DataFrame(g.apply(lambda x: x.sample(g.size().min()).reset_index(drop=True)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "891c475d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "data['label'].value_counts().plot(kind=\"bar\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0982986a",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['label_int'] = LabelEncoder().fit_transform(data['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3456fc8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_LABELS = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ee75f2bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_split, test_split = train_test_split(data, train_size = 0.8)\n",
    "train_split, val_split = train_test_split(train_split, train_size = 0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "640f2c15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4414\n",
      "1227\n",
      "491\n"
     ]
    }
   ],
   "source": [
    "print(len(train_split))\n",
    "print(len(test_split))\n",
    "print(len(val_split))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4e25163c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.DataFrame({\n",
    "    \"label\": train_split.label_int.values,\n",
    "    \"text\": train_split.text_clean.values\n",
    "})\n",
    "\n",
    "test_df = pd.DataFrame({\n",
    "    \"label\": test_split.label_int.values,\n",
    "    \"text\": test_split.text_clean.values\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b8aa7ed6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# format it to dataset dictionary\n",
    "train_df = datasets.Dataset.from_dict(train_df)\n",
    "test_df = datasets.Dataset.from_dict(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "42c03d59",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_dict = datasets.DatasetDict({\"train\":train_df, \"test\":test_df})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d982e0c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['label', 'text'],\n",
       "        num_rows: 4414\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['label', 'text'],\n",
       "        num_rows: 1227\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb87f58f",
   "metadata": {},
   "source": [
    "## Create embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "e85576e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting sentencepiece\n",
      "  Downloading sentencepiece-0.2.0-cp39-cp39-win_amd64.whl (991 kB)\n",
      "Installing collected packages: sentencepiece\n",
      "Successfully installed sentencepiece-0.2.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "56a5573f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sanaz\\anaconda3\\lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# alwayes define tokenizer for the embedding section\n",
    "tokenizer = XLNetTokenizer.from_pretrained(\"xlnet-base-cased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "eecdfcd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"text\"], padding = \"max_length\", max_length = 128, truncation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a9bc3f89",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a47fa0db283f4f239c911954f00e6b8d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/4414 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dfbb9dceaccb4a4e86d988bda792c987",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1227 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenized_datasets = dataset_dict.map(tokenize_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c5300efc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['label', 'text', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "        num_rows: 4414\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['label', 'text', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "        num_rows: 1227\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d9a37181",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  fingers of fury!\n"
     ]
    }
   ],
   "source": [
    "print(tokenized_datasets['train']['text'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "19f93a27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 3267, 20, 18439, 136, 4, 3]\n"
     ]
    }
   ],
   "source": [
    "print(tokenized_datasets['train']['input_ids'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2ffd7401",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<pad>'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b29bb46a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 0, 0, 0, 0, 0, 2]\n"
     ]
    }
   ],
   "source": [
    "print(tokenized_datasets['train']['token_type_ids'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "26ac183a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "print(tokenized_datasets['train']['attention_mask'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "dd111f9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "small_train_dataset = tokenized_datasets[\"train\"].shuffle(seed=42).select(range(100))\n",
    "small_eval_dataset = tokenized_datasets[\"test\"].shuffle(seed=42).select(range(100))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92f6e099",
   "metadata": {},
   "source": [
    "## Fine tune a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "788a9093",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of XLNetForSequenceClassification were not initialized from the model checkpoint at xlnet-base-cased and are newly initialized: ['logits_proj.bias', 'logits_proj.weight', 'sequence_summary.summary.bias', 'sequence_summary.summary.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = XLNetForSequenceClassification.from_pretrained('xlnet-base-cased', \n",
    "                                                       num_labels=NUM_LABELS, \n",
    "                                                       id2label={0: 'anger', 1: 'fear', 2: 'joy', 3: 'sadness'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c61771f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "metric = evaluate.load(\"accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "55d85675",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    return metric.compute(predictions=predictions, references=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "6e28147a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting accelerate\n",
      "  Downloading accelerate-0.33.0-py3-none-any.whl (315 kB)\n",
      "Requirement already satisfied: numpy<2.0.0,>=1.17 in c:\\users\\sanaz\\anaconda3\\lib\\site-packages (from accelerate) (1.26.4)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in c:\\users\\sanaz\\anaconda3\\lib\\site-packages (from accelerate) (0.4.4)\n",
      "Requirement already satisfied: torch>=1.10.0 in c:\\users\\sanaz\\anaconda3\\lib\\site-packages (from accelerate) (2.4.0)\n",
      "Requirement already satisfied: psutil in c:\\users\\sanaz\\anaconda3\\lib\\site-packages (from accelerate) (5.8.0)\n",
      "Requirement already satisfied: huggingface-hub>=0.21.0 in c:\\users\\sanaz\\anaconda3\\lib\\site-packages (from accelerate) (0.24.5)\n",
      "Requirement already satisfied: pyyaml in c:\\users\\sanaz\\anaconda3\\lib\\site-packages (from accelerate) (6.0)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\sanaz\\anaconda3\\lib\\site-packages (from accelerate) (24.1)\n",
      "Requirement already satisfied: requests in c:\\users\\sanaz\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.21.0->accelerate) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in c:\\users\\sanaz\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.21.0->accelerate) (4.66.5)\n",
      "Requirement already satisfied: filelock in c:\\users\\sanaz\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.21.0->accelerate) (3.6.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\sanaz\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.21.0->accelerate) (4.12.2)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\sanaz\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.21.0->accelerate) (2024.5.0)\n",
      "Requirement already satisfied: networkx in c:\\users\\sanaz\\anaconda3\\lib\\site-packages (from torch>=1.10.0->accelerate) (2.7.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\sanaz\\anaconda3\\lib\\site-packages (from torch>=1.10.0->accelerate) (2.11.3)\n",
      "Requirement already satisfied: sympy in c:\\users\\sanaz\\anaconda3\\lib\\site-packages (from torch>=1.10.0->accelerate) (1.10.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\sanaz\\anaconda3\\lib\\site-packages (from tqdm>=4.42.1->huggingface-hub>=0.21.0->accelerate) (0.4.4)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in c:\\users\\sanaz\\anaconda3\\lib\\site-packages (from jinja2->torch>=1.10.0->accelerate) (2.0.1)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\sanaz\\anaconda3\\lib\\site-packages (from requests->huggingface-hub>=0.21.0->accelerate) (1.26.9)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\sanaz\\anaconda3\\lib\\site-packages (from requests->huggingface-hub>=0.21.0->accelerate) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\sanaz\\anaconda3\\lib\\site-packages (from requests->huggingface-hub>=0.21.0->accelerate) (3.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\sanaz\\anaconda3\\lib\\site-packages (from requests->huggingface-hub>=0.21.0->accelerate) (2021.10.8)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\users\\sanaz\\anaconda3\\lib\\site-packages (from sympy->torch>=1.10.0->accelerate) (1.2.1)\n",
      "Installing collected packages: accelerate\n",
      "Successfully installed accelerate-0.33.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install accelerate -U"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "7c1762b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sanaz\\anaconda3\\lib\\site-packages\\transformers\\training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "training_args = TrainingArguments(output_dir=\"test_trainer\", evaluation_strategy=\"epoch\", num_train_epochs=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "afd5551e",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model, \n",
    "    args=training_args,\n",
    "    train_dataset=small_train_dataset,\n",
    "    eval_dataset=small_eval_dataset,\n",
    "    compute_metrics=compute_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "f1ff7e22",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='40' max='39' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [39/39 04:59, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.415917</td>\n",
       "      <td>0.270000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.386812</td>\n",
       "      <td>0.270000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "ValueError",
     "evalue": "You are trying to save a non contiguous tensor: `transformer.layer.0.ff.layer_1.weight` which is not allowed. It either means you are trying to save tensors which are reference of each other in which case it's recommended to save only the full tensors, and reslice at load time, or simply call `.contiguous()` on your tensor to pack it before saving.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[1;32mIn [37]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\transformers\\trainer.py:1948\u001b[0m, in \u001b[0;36mTrainer.train\u001b[1;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[0;32m   1946\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[0;32m   1947\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1948\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1949\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1950\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1951\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1952\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1953\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\transformers\\trainer.py:2366\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[1;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[0;32m   2363\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mepoch \u001b[38;5;241m=\u001b[39m epoch \u001b[38;5;241m+\u001b[39m (step \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m steps_skipped) \u001b[38;5;241m/\u001b[39m steps_in_epoch\n\u001b[0;32m   2364\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_handler\u001b[38;5;241m.\u001b[39mon_step_end(args, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol)\n\u001b[1;32m-> 2366\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_maybe_log_save_evaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtr_loss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_norm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2367\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   2368\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_handler\u001b[38;5;241m.\u001b[39mon_substep_end(args, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\transformers\\trainer.py:2817\u001b[0m, in \u001b[0;36mTrainer._maybe_log_save_evaluate\u001b[1;34m(self, tr_loss, grad_norm, model, trial, epoch, ignore_keys_for_eval)\u001b[0m\n\u001b[0;32m   2814\u001b[0m     metrics \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_evaluate(trial, ignore_keys_for_eval)\n\u001b[0;32m   2816\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol\u001b[38;5;241m.\u001b[39mshould_save:\n\u001b[1;32m-> 2817\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_save_checkpoint\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetrics\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetrics\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2818\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_handler\u001b[38;5;241m.\u001b[39mon_save(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\transformers\\trainer.py:2896\u001b[0m, in \u001b[0;36mTrainer._save_checkpoint\u001b[1;34m(self, model, trial, metrics)\u001b[0m\n\u001b[0;32m   2894\u001b[0m run_dir \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_output_dir(trial\u001b[38;5;241m=\u001b[39mtrial)\n\u001b[0;32m   2895\u001b[0m output_dir \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(run_dir, checkpoint_folder)\n\u001b[1;32m-> 2896\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_internal_call\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m   2898\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39msave_only_model:\n\u001b[0;32m   2899\u001b[0m     \u001b[38;5;66;03m# Save optimizer and scheduler\u001b[39;00m\n\u001b[0;32m   2900\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_save_optimizer_and_scheduler(output_dir)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\transformers\\trainer.py:3464\u001b[0m, in \u001b[0;36mTrainer.save_model\u001b[1;34m(self, output_dir, _internal_call)\u001b[0m\n\u001b[0;32m   3461\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_wrapped\u001b[38;5;241m.\u001b[39msave_checkpoint(output_dir)\n\u001b[0;32m   3463\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mshould_save:\n\u001b[1;32m-> 3464\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_save\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput_dir\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3466\u001b[0m \u001b[38;5;66;03m# Push to the Hub when `save_model` is called by the user.\u001b[39;00m\n\u001b[0;32m   3467\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mpush_to_hub \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _internal_call:\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\transformers\\trainer.py:3535\u001b[0m, in \u001b[0;36mTrainer._save\u001b[1;34m(self, output_dir, state_dict)\u001b[0m\n\u001b[0;32m   3533\u001b[0m             torch\u001b[38;5;241m.\u001b[39msave(state_dict, os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(output_dir, WEIGHTS_NAME))\n\u001b[0;32m   3534\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 3535\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   3536\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstate_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstate_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msafe_serialization\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave_safetensors\u001b[49m\n\u001b[0;32m   3537\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3539\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtokenizer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   3540\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtokenizer\u001b[38;5;241m.\u001b[39msave_pretrained(output_dir)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\transformers\\modeling_utils.py:2771\u001b[0m, in \u001b[0;36mPreTrainedModel.save_pretrained\u001b[1;34m(self, save_directory, is_main_process, state_dict, save_function, push_to_hub, max_shard_size, safe_serialization, variant, token, save_peft_format, **kwargs)\u001b[0m\n\u001b[0;32m   2766\u001b[0m     gc\u001b[38;5;241m.\u001b[39mcollect()\n\u001b[0;32m   2768\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m safe_serialization:\n\u001b[0;32m   2769\u001b[0m     \u001b[38;5;66;03m# At some point we will need to deal better with save_function (used for TPU and other distributed\u001b[39;00m\n\u001b[0;32m   2770\u001b[0m     \u001b[38;5;66;03m# joyfulness), but for now this enough.\u001b[39;00m\n\u001b[1;32m-> 2771\u001b[0m     \u001b[43msafe_save_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mshard\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43msave_directory\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshard_file\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mformat\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpt\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2772\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   2773\u001b[0m     save_function(shard, os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(save_directory, shard_file))\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\safetensors\\torch.py:286\u001b[0m, in \u001b[0;36msave_file\u001b[1;34m(tensors, filename, metadata)\u001b[0m\n\u001b[0;32m    255\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msave_file\u001b[39m(\n\u001b[0;32m    256\u001b[0m     tensors: Dict[\u001b[38;5;28mstr\u001b[39m, torch\u001b[38;5;241m.\u001b[39mTensor],\n\u001b[0;32m    257\u001b[0m     filename: Union[\u001b[38;5;28mstr\u001b[39m, os\u001b[38;5;241m.\u001b[39mPathLike],\n\u001b[0;32m    258\u001b[0m     metadata: Optional[Dict[\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mstr\u001b[39m]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    259\u001b[0m ):\n\u001b[0;32m    260\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    261\u001b[0m \u001b[38;5;124;03m    Saves a dictionary of tensors into raw bytes in safetensors format.\u001b[39;00m\n\u001b[0;32m    262\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    284\u001b[0m \u001b[38;5;124;03m    ```\u001b[39;00m\n\u001b[0;32m    285\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 286\u001b[0m     serialize_file(\u001b[43m_flatten\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m)\u001b[49m, filename, metadata\u001b[38;5;241m=\u001b[39mmetadata)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\safetensors\\torch.py:496\u001b[0m, in \u001b[0;36m_flatten\u001b[1;34m(tensors)\u001b[0m\n\u001b[0;32m    487\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m failing:\n\u001b[0;32m    488\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m    489\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[0;32m    490\u001b[0m \u001b[38;5;124m        Some tensors share memory, this will lead to duplicate memory on disk and potential differences when loading them again: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfailing\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    493\u001b[0m \u001b[38;5;124m        \u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[0;32m    494\u001b[0m     )\n\u001b[1;32m--> 496\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m {\n\u001b[0;32m    497\u001b[0m     k: {\n\u001b[0;32m    498\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mstr\u001b[39m(v\u001b[38;5;241m.\u001b[39mdtype)\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m],\n\u001b[0;32m    499\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mshape\u001b[39m\u001b[38;5;124m\"\u001b[39m: v\u001b[38;5;241m.\u001b[39mshape,\n\u001b[0;32m    500\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata\u001b[39m\u001b[38;5;124m\"\u001b[39m: _tobytes(v, k),\n\u001b[0;32m    501\u001b[0m     }\n\u001b[0;32m    502\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m tensors\u001b[38;5;241m.\u001b[39mitems()\n\u001b[0;32m    503\u001b[0m }\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\safetensors\\torch.py:500\u001b[0m, in \u001b[0;36m<dictcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    487\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m failing:\n\u001b[0;32m    488\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m    489\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[0;32m    490\u001b[0m \u001b[38;5;124m        Some tensors share memory, this will lead to duplicate memory on disk and potential differences when loading them again: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfailing\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    493\u001b[0m \u001b[38;5;124m        \u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[0;32m    494\u001b[0m     )\n\u001b[0;32m    496\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m {\n\u001b[0;32m    497\u001b[0m     k: {\n\u001b[0;32m    498\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mstr\u001b[39m(v\u001b[38;5;241m.\u001b[39mdtype)\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m],\n\u001b[0;32m    499\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mshape\u001b[39m\u001b[38;5;124m\"\u001b[39m: v\u001b[38;5;241m.\u001b[39mshape,\n\u001b[1;32m--> 500\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[43m_tobytes\u001b[49m\u001b[43m(\u001b[49m\u001b[43mv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[0;32m    501\u001b[0m     }\n\u001b[0;32m    502\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m tensors\u001b[38;5;241m.\u001b[39mitems()\n\u001b[0;32m    503\u001b[0m }\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\safetensors\\torch.py:414\u001b[0m, in \u001b[0;36m_tobytes\u001b[1;34m(tensor, name)\u001b[0m\n\u001b[0;32m    407\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    408\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou are trying to save a sparse tensor: `\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m` which this library does not support.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    409\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m You can make it a dense tensor before saving with `.to_dense()` but be aware this might\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    410\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m make a much larger file than needed.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    411\u001b[0m     )\n\u001b[0;32m    413\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m tensor\u001b[38;5;241m.\u001b[39mis_contiguous():\n\u001b[1;32m--> 414\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    415\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou are trying to save a non contiguous tensor: `\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m` which is not allowed. It either means you\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    416\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m are trying to save tensors which are reference of each other in which case it\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ms recommended to save\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    417\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m only the full tensors, and reslice at load time, or simply call `.contiguous()` on your tensor to\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    418\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m pack it before saving.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    419\u001b[0m     )\n\u001b[0;32m    420\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m tensor\u001b[38;5;241m.\u001b[39mdevice\u001b[38;5;241m.\u001b[39mtype \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    421\u001b[0m     \u001b[38;5;66;03m# Moving tensor to cpu before saving\u001b[39;00m\n\u001b[0;32m    422\u001b[0m     tensor \u001b[38;5;241m=\u001b[39m tensor\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mValueError\u001b[0m: You are trying to save a non contiguous tensor: `transformer.layer.0.ff.layer_1.weight` which is not allowed. It either means you are trying to save tensors which are reference of each other in which case it's recommended to save only the full tensors, and reslice at load time, or simply call `.contiguous()` on your tensor to pack it before saving."
     ]
    }
   ],
   "source": [
    "trainer.train() # use trainer function from huggingface"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8afb59b6",
   "metadata": {},
   "source": [
    "## Evaluate Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f4d453e",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d728388e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained(\"fine_tuned_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62727f1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "fine_tuned_model = XLNetForSequenceClassification.from_pretrained(\"fine_tuned_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "890cf0f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = pipeline(\"text-classification\", fine_tuned_model, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38987734",
   "metadata": {},
   "outputs": [],
   "source": [
    "rand_int = random.randint(0, len(val_split))\n",
    "print(val_split['text_clean'][rand_int])\n",
    "answer = clf(val_split['text_clean'][rand_int], top_k=None)\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1f867d4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
